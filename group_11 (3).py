# -*- coding: utf-8 -*-
"""GROUP_11

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_x-N6jpeq1d3jgJcRObmRXj005FN9z8H
"""

import math

class Variable:

    def __init__(self, value, _prevNode=(), _op='', label=''): # added label arg
        self.value = value
        self.grad = 0
        self._autograd = lambda: None
        self._prev = set(_prevNode)
        self._op = _op # the op that produced this node, for graphviz / debugging / etc
        self.label = label # i added the label thingie here so my draw box will have the label thingie AHahhaha can remove once we ss the draw box?

    def __add__(self, other):
        if isinstance(other, Variable):
            other = other
        else:
            other = Variable(other)

        out = Variable(self.value + other.value, (self, other), '+')

        def _autograd():
            self.grad += out.grad
            other.grad += out.grad
        out._autograd = _autograd

        return out

    def __mul__(self, other):

        if isinstance(other, Variable):
            other = other
        else:
            other = Variable(other)

        out = Variable(self.value * other.value, (self, other), '*')

        def _autograd():
            self.grad += other.value * out.grad
            other.grad += self.value * out.grad

        out._autograd = _autograd
        return out


    def __pow__(self, other):

        if isinstance(other, (int,float)):
            other=other
        else:
            print("only supporting integers and floats")

        out = Variable(self.value**other, (self,), f'**{other}')

        def _autograd():
            self.grad += (other * self.value**(other-1)) * out.grad
        out._autograd = _autograd

        return out

    def relu(self):
        out = Variable(0 if self.value < 0 else self.value, (self,), 'ReLU')

        def _autograd():
            self.grad += (out.value > 0) * out.grad
        out._autograd = _autograd

        return out

    def exp(self):
        out = Variable(math.exp(self.value), (self,), 'exp')

        def _autograd():
            self.grad += (out.value) * out.grad
        out._autograd = _autograd

        return out

    def tanh(self):
        a = (math.exp(2*self.value)-1)/(math.exp(2*self.value)+1)
        out = Variable(a, (self,), 'tanh')

        def _autograd():
            self.grad += (1-a**2) * out.grad
        out._autograd = _autograd

        return out

    def sigmoid(self):
      out = Variable(1*(1+math.exp(-1*self.value))**-1, (self,), 'sigmoid')

      def _autograd():
        self.grad += (math.exp(-1*self.value)*(1+math.exp(-1*self.value))**-2)
      out._autograd = _autograd
      return out

    def sin(self):
        out = Variable(math.sin(self.value), (self,), 'sin')

        def _autograd():
            self.grad += math.cos(self.value) * out.grad
        out._autograd = _autograd
        return out

    def cos(self):
        out = Variable(math.cos(self.value), (self,), 'cos')

        def _autograd():
            self.grad += -1 * math.sin(self.value) * out.grad
        out._autograd = _autograd
        return out

    def tan(self):
        out = Variable(math.tan(self.value), (self,), 'tan')

        def _autograd():
            self.grad += (math.cos(self.value)**2 + math.sin(self.value)**2)*(math.cos(self.value)**2)**-1
        out._autograd = _autograd
        return out


  #AI copilot prompt: generate a topological backward code
    def autograd(self):
        order = []
        visited = set()

        #topo sort to ensure that arrows are pointing in one direction and we can find the gradient whenw we apply autograd

        def build_order(node):
            if node not in visited:
                visited.add(node)
                for parent in node._prev:
                    build_order(parent)
                order.append(node)
        build_order(self)

            #apply chain rule to get gradient of previous nodes

        self.grad = 1
        for node in reversed(order):
            node._autograd()

    def __neg__(self): # -self
        return self * -1

    def __radd__(self, other): # other + self
        return self + other

    def __sub__(self, other): # self - other
        return self + (-other)

    def __rsub__(self, other): # other - self
        return other + (-self)

    def __rmul__(self, other): # other * self
        #print('hihi')
        return self * other

    def __truediv__(self, other): # self / other
        #print('hi')
        return self * other**-1

    def __rtruediv__(self, other): # other / self
        return other * self**-1

    def __repr__(self):
        return f"Variable(value={self.value}, grad={self.grad})"

#testing sin and cos
#draw box for trigo functions
#sin
x = Variable(2.0)
y = x.sin()
y.autograd()

#cos
x = Variable(2.0)
y = x.cos()
y.autograd()

#tan
x = Variable(2.0)
y = x.tan()
y.autograd()

#trigo with chain rule
x = Variable(2.0, label = 'x')
y = Variable(3.0, label = 'y')
z = x * y; z.label = 'z'
a = z.sin() + z.cos() + z.tan(); a.label='a'
a.autograd()

#testing sigmoid
x = Variable(2.0, label = 'x')
y = x.sigmoid(); y.label='y'
y.autograd()

#testing chain rule with sigmoid
x = Variable(2.0, label = 'x')
y = 2*x; y.label='y'
z = y.sigmoid();z.label='z'
a = 3*x*z;a.label='a'
a.autograd()

#testing tanh
x = Variable(2.0, label='x')
y = x.tanh(); y.label='y'
y.autograd()

#chain rule with tanh
x = Variable(2.0, label='x')
y = Variable(3.0, label='y')
z = x.tanh(); z.label='z'
a = y.tanh(); a.label='a'
b = a + z; b.label='b'
b.autograd()

#testing with relu
x = Variable(1.0, label='x')
y = (x * 2 + 1).relu();y.label='y'
y.autograd()

x=Variable(2.0, label='x')
y=x.relu();y.label='y'
y.autograd()

# testing basic mul and add

x = Variable(-5.0, label='x')
y = Variable(2.0, label='y')
a = x * y; a.label = 'a'
b = x + y; b.label = 'b'
c = a * b; c.label = 'c'
c.autograd()
print(c.autograd)
print(x.autograd)

# testing exp
a = Variable(2.0)
b = Variable(3.0)
c = a.exp() * b
c.autograd()
print(c.autograd)
print(a.autograd)
print(b.autograd)

# testing power
x = Variable(2.0, label = 'x')
a = x**2; a.label = 'a'
a.autograd()
print(a.autograd)
print(x.autograd)

import random
# Define a Module class
class MyModule:

    # Method to zero out gradients
    def reset_gradients(self):
        for parameter in self.get_parameters():
            parameter.gradient = 0

    # Method to get parameters
    def get_parameters(self):
        return []

# Define a Neuron class
class MyNeuron(MyModule):

    # Constructor
    def __init__(self, input_size, nonlinearity=True):
        # Initialize weights randomly
        self.weights = [Variable(random.uniform(-1, 1)) for _ in range(input_size)]
        # Initialize bias
        self.bias = Variable(0)
        # Store nonlinearity flag
        self.nonlinearity = nonlinearity

    def __call__(self,x):
        result = sum((wi*xi for wi, xi in zip(self.weights, x)), self.bias)
        return result.relu() if self.nonlinearity else result

    # Forward pass

    def forward(self, input_vector):
        # Calculate the activation
        activation = sum((weight * input_value for weight, input_value in zip(self.weights, input_vector)), self.bias)
        # Apply nonlinearity if enabled
        return activation.relu() if self.nonlinearity else activation

    # Method to get parameters
    def get_parameters(self):
        return self.weights + [self.bias]

    # Representation
    def __repr__(self):
        return f"{'ReLU' if self.nonlinearity else 'Linear'}Neuron({len(self.weights)})"

# Define a Layer class
class MyLayer(MyModule):

    # Constructor
    def __init__(self, input_size, output_size, **kwargs):
        # Create neurons
        self.neurons = [MyNeuron(input_size, **kwargs) for _ in range(output_size)]

    def __call__(self, x):
        result = [n(x) for n in self.neurons]
        return result[0] if len(result) == 1 else result
    # Forward pass
    def forward(self, input_vector):
        output = [neuron.forward(input_vector) for neuron in self.neurons]
        return output[0] if len(output) == 1 else output

    # Method to get parameters
    def get_parameters(self):
        return [parameter for neuron in self.neurons for parameter in neuron.get_parameters()]

    # Representation
    def __repr__(self):
        return f"Layer of [{', '.join(str(neuron) for neuron in self.neurons)}]"

# Define a Multi-Layer Perceptron (MLP) class
class MyMLP(MyModule):

    # Constructor
    def __init__(self, input_size, output_sizes):
        # Compute layer sizes
        layer_sizes = [input_size] + output_sizes
        # Create layers
        self.layers = [MyLayer(layer_sizes[i], layer_sizes[i + 1], nonlinearity=i != len(output_sizes) - 1) for i in range(len(output_sizes))]

    def __call__(self,x):
        for layer in self.layers:
          x = layer(x)
        return x

    # Forward pass

    def forward(self, input_vector):
        for layer in self.layers:
            input_vector = layer.forward(input_vector)
        return input_vector


    # Method to get parameters
    def get_parameters(self):
        return [parameter for layer in self.layers for parameter in layer.get_parameters()]

    # Representation
    def __repr__(self):
        return f"MLP of [{', '.join(str(layer) for layer in self.layers)}]"

x = [4.0, 7.0, -1.0]
n = MyMLP(3,[4,4,1])
n(x)

x_1 = [
    [6.0, 9.0, -1.0],
    [4.0, -1.0, 2.0],
    [0.5, 1.5, 1.0],
    [3.0, 2.0, -1.0]
]
y_1 = [1.0, -1.0, -1.0, 1.0]
ypred = [n(x) for x in x_1]
ypred

for k in range(20):

  ypred = [n(x) for x in x_1]
  loss = sum((yout-ygt)**2 for ygt, yout in zip(y_1,ypred))

  #backward pass
  for p in n.get_parameters():
    p.grad = 0.0
  loss.autograd()

  #update
  for p in n.get_parameters():
    p.value += -0.05 * p.grad
  print(k, loss.value)

